{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Azure ML Pipeline - Autoencoder Training & Registration\r\n",
    "This notebook demonstrates creation & execution of an Azure ML pipeline designed to load data from an attached ADLS Gen 2 datastore, split the data into test/train sets, train an autoencoder using TensorFlow & Keras, then run an A/B test to check whether the new model performs better than the current 'best' registered model. If the new model is found to outperform the current best performer, it is registered to the AML workspace. This notebook was built as part of a larger MLOps solution where this pipeline is deployed and triggered via Azure DevOps and the best performing model is published to a real-time endpoint"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Import required packages\r\n",
    "from azureml.core import Workspace, Experiment, Datastore, Environment, Dataset\r\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute, DataFactoryCompute\r\n",
    "from azureml.core.compute_target import ComputeTargetException\r\n",
    "from azureml.core.runconfig import RunConfiguration\r\n",
    "from azureml.core.conda_dependencies import CondaDependencies\r\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\r\n",
    "from azureml.pipeline.core import Pipeline, PipelineParameter, PipelineData\r\n",
    "from azureml.pipeline.steps import PythonScriptStep\r\n",
    "from azureml.pipeline.core import PipelineParameter, PipelineData\r\n",
    "from azureml.data.output_dataset_config import OutputTabularDatasetConfig, OutputDatasetConfig, OutputFileDatasetConfig\r\n",
    "from azureml.data.datapath import DataPath\r\n",
    "from azureml.data.data_reference import DataReference\r\n",
    "from azureml.data.sql_data_reference import SqlDataReference\r\n",
    "from azureml.pipeline.steps import DataTransferStep\r\n",
    "import logging"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Connect to Azure ML Workspace, Provision Compute Resources, and get References to Datastores\n",
    "Connect to workspace using config associated config file. Get a reference to you pre-existing AML compute cluster or provision a new cluster to facilitate processing. Finally, get references to your default blob datastore."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Connect to AML Workspace\r\n",
    "ws = Workspace.from_config()\r\n",
    "\r\n",
    "#Select AML Compute Cluster\r\n",
    "cpu_cluster_name = 'cpucluster'\r\n",
    "\r\n",
    "# Verify that cluster does not exist already\r\n",
    "try:\r\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\r\n",
    "    print('Found an existing cluster, using it instead.')\r\n",
    "except ComputeTargetException:\r\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D3_V2',\r\n",
    "                                                           min_nodes=0,\r\n",
    "                                                           max_nodes=1)\r\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\r\n",
    "    cpu_cluster.wait_for_completion(show_output=True)\r\n",
    "    \r\n",
    "#Get default datastore\r\n",
    "default_ds = ws.get_default_datastore()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Run Configuration\r\n",
    "The RunConfiguration defines the environment used across all python steps. You can optionally add additional conda or pip packages to be added to your environment. [More details here.](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py)\r\n",
    "~~~\r\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['requests'])\r\n",
    "run_config.environment.python.conda_dependencies.add_pip_package('azureml-opendatasets')\r\n",
    "~~~\r\n",
    "Here, we also register the environment to the AML workspace so that it can be used for future retraining and inferencing operations."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "run_config = RunConfiguration()\r\n",
    "run_config.docker.use_docker = True\r\n",
    "run_config.environment = Environment(name='tf_keras_autoencoder_env')\r\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\r\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create()\r\n",
    "run_config.environment.python.conda_dependencies.set_pip_requirements([\r\n",
    "    'requests==2.26.0',\r\n",
    "    'pandas==0.25.3',\r\n",
    "    'numpy==1.19.2',\r\n",
    "    'scikit-learn==0.22.2.post1',\r\n",
    "    'joblib==0.14.1',\r\n",
    "    'h5py==3.1.0',\r\n",
    "    'tensorflow==2.6.0',\r\n",
    "    'keras==2.6.0',\r\n",
    "    'azureml-defaults==1.33.0',\r\n",
    "    'matplotlib'\r\n",
    "])\r\n",
    "run_config.environment.python.conda_dependencies.set_python_version('3.8.10')\r\n",
    "\r\n",
    "#Register environment for reuse \r\n",
    "run_config.environment.register(ws)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define Output Datasets\n",
    "Below we define the configuration for datasets that will be passed between steps in our pipeline. Note, in all cases we specify the datastore that should hold the datasets and whether they should be registered following step completion or not. This can optionally be disabled by removing the register_on_complete() call. upload_file_dataset is intended to hold the data within an uploaded CSV file and processed_dataset will contain our uploaded data post-processing."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "autoencoder_raw_data = OutputFileDatasetConfig(name='Autoencoder_Raw_Data', destination=(default_ds, 'autoencoder_raw_data/{run-id}')).read_delimited_files().register_on_complete(name='Autoencoder_Raw_Data')\r\n",
    "autoencoder_training_data = OutputFileDatasetConfig(name='Autoencoder_Training_Data', destination=(default_ds, 'autoencoder_training_data/{run-id}')).read_delimited_files().register_on_complete(name='Autoencoder_Training_Data')\r\n",
    "autoencoder_testing_data = OutputFileDatasetConfig(name='Autoencoder_Testing_Data', destination=(default_ds, 'autoencoder_testing_data/{run-id}')).read_delimited_files().register_on_complete(name='Autoencoder_Testing_Data')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define Pipeline Data\r\n",
    "Pipeline data represents intermediate files/data in an Azure Machine Learning pipeline that needs to be shuttled between steps. In our case we are fitting a MinMax scaler in an early step that needs to be consumed in a later step. Additionall, we will register the trained scaler and model artifacts in the final step if our model performs better than the current model. For both cases, we use `PipelineData` objects to facilitate this motion of files between steps. [More details can be found here.](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelinedata?view=azure-ml-py)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "split_to_train_pipeline_data = PipelineData(name='Autoencoder_SplitScale_Outputs', datastore=default_ds)\r\n",
    "train_to_evaluate_pipeline_data = PipelineData(name='Autoencoder_Training_Outputs', datastore=default_ds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define Pipeline Parameters\r\n",
    "PipelineParameter objects serve as variable inputs to an Azure ML pipeline and can be specified at runtime. Below we specify the number of epochs (`num_epochs`) and batch size (`batch_size`) as variable parameters which can be passed into the pipeline at runtime."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_epochs = PipelineParameter(name='num_epochs', default_value=10)\r\n",
    "batch_size = PipelineParameter(name='batch_size', default_value=10)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define Pipeline Steps\r\n",
    "The pipeline below consists of four distinct steps all of which execute an associated python script located in the `./pipeline_script_steps` dir. First, we call `get_data.py` and retrieve data from the registered ADLS Gen 2 datastore and register this dataset as `Autoencoder_Raw_Data`. From here we run `split_and_scale.py` which normalizes the raw data using a `MinMaxScaler` and then splits into test and train datasets - both of which are subsequently registered. Then, we pass the test and training datasets into a step that runs `train_model.py` which trains the autoencoder and computes as Mean Squared Error (MSE) metric. Finally, the final step executes `evaluate_and_register.py` which compares the MSE of our model to the MSE of the currently registered autoencoder. If the current model performs better, or no model has been registered to-date, the model is registered in the workspace."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Get raw data from registered ADLS Gen2 datastore\r\n",
    "#Register tabular dataset after retrieval\r\n",
    "get_data_step = PythonScriptStep(\r\n",
    "    name='Get Data from ADLS Gen2',\r\n",
    "    script_name='get_data.py',\r\n",
    "    arguments =['--autoencoder_raw_data', autoencoder_raw_data],\r\n",
    "    outputs=[autoencoder_raw_data],\r\n",
    "    compute_target=cpu_cluster,\r\n",
    "    source_directory='./pipeline_step_scripts',\r\n",
    "    allow_reuse=False,\r\n",
    "    runconfig=run_config\r\n",
    ")\r\n",
    "\r\n",
    "#Normalize the raw data using a MinMaxScaler\r\n",
    "#and then split into test and train datasets\r\n",
    "split_scale_step = PythonScriptStep(\r\n",
    "    name='Split and Scale Raw Data',\r\n",
    "    script_name='split_and_scale.py',\r\n",
    "    arguments =['--autoencoder_training_data', autoencoder_training_data,\r\n",
    "                '--autoencoder_testing_data', autoencoder_testing_data,\r\n",
    "                '--split_to_train_pipeline_data', split_to_train_pipeline_data],\r\n",
    "    inputs=[autoencoder_raw_data.as_input(name='Autoencoder_Raw_Data')],\r\n",
    "    outputs=[autoencoder_training_data, autoencoder_testing_data, split_to_train_pipeline_data],\r\n",
    "    compute_target=cpu_cluster,\r\n",
    "    source_directory='./pipeline_step_scripts',\r\n",
    "    allow_reuse=False,\r\n",
    "    runconfig=run_config\r\n",
    ")\r\n",
    "\r\n",
    "#Train autoencoder using raw data as an input\r\n",
    "#Raw data will be preprocessed and registered as train/test datasets\r\n",
    "#Scaler and train autoencoder will be saved out\r\n",
    "train_model_step = PythonScriptStep(\r\n",
    "    name='Train TF/Keras Autoencoder',\r\n",
    "    script_name='train_model.py',\r\n",
    "    arguments =[\r\n",
    "                '--train_to_evaluate_pipeline_data', train_to_evaluate_pipeline_data,\r\n",
    "                '--split_to_train_pipeline_data', split_to_train_pipeline_data,\r\n",
    "                '--num_epochs', num_epochs,\r\n",
    "                '--batch_size', batch_size],\r\n",
    "    inputs=[autoencoder_training_data.as_input(name='Autoencoder_Training_Data'),\r\n",
    "            autoencoder_testing_data.as_input(name='Autoencoder_Testing_Data'),\r\n",
    "            split_to_train_pipeline_data.as_input('Autoencoder_SplitScale_Outputs')\r\n",
    "           ],\r\n",
    "    outputs=[train_to_evaluate_pipeline_data],\r\n",
    "    compute_target=cpu_cluster,\r\n",
    "    source_directory='./pipeline_step_scripts',\r\n",
    "    allow_reuse=False,\r\n",
    "    runconfig=run_config\r\n",
    ")\r\n",
    "\r\n",
    "#Evaluate and register model here\r\n",
    "#Compare metrics from current model and register if better than current\r\n",
    "#best model\r\n",
    "evaluate_and_register_step = PythonScriptStep(\r\n",
    "    name='Evaluate and Register Autoencoder',\r\n",
    "    script_name='evaluate_and_register.py',\r\n",
    "    arguments=['--autoencoder_training_outputs', train_to_evaluate_pipeline_data],\r\n",
    "    inputs=[autoencoder_training_data.as_input(name='Autoencoder_Training_Data'),\r\n",
    "            autoencoder_testing_data.as_input(name='Autoencoder_Testing_Data'),\r\n",
    "            train_to_evaluate_pipeline_data.as_input('Autoencoder_Training_Outputs')],\r\n",
    "    compute_target=cpu_cluster,\r\n",
    "    source_directory='./pipeline_step_scripts',\r\n",
    "    allow_reuse=False,\r\n",
    "    runconfig=run_config\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Pipeline\n",
    "Create an Azure ML Pipeline by specifying the steps to be executed. Note: based on the dataset dependencies between steps, exection occurs logically such that no step will execute unless all of the necessary input datasets have been generated."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[get_data_step, split_scale_step, train_model_step, evaluate_and_register_step])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Publish Pipeline\n",
    "Create a published version of your pipeline that can be triggered via an authenticated REST API request."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "published_pipeline = pipeline.publish(name = 'Autoencoder_Training_Registration_Pipeline',\r\n",
    "                                     description = 'Pipeline to load/register IoT telemetry data from ADLS Gen2, train a Tensorflow/Keras autoencoder for anomaly detection, and register the trained model if it performs better than the current best model.',\r\n",
    "                                     continue_on_step_failure = False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Optional: Submit a Pipeline Run\n",
    "You can create a new experiment  (logical container for pipeline runs) and execute the pipeline. Note: the values of pipeline parameters can be modified when submitting a new run."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "experiment = Experiment(ws, 'sample-pipeline-run')\r\n",
    "run = experiment.submit(pipeline)\r\n",
    "run.wait_for_completion(show_output=True)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}